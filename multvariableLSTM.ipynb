{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 11 14:40:27 2020\n",
    "@author: revan\n",
    "\n",
    "Aprimoreted on Monday April 01 21:53:03 2024\n",
    "@autor: Janderson Hillebrecht\n",
    "\"\"\"\n",
    "\n",
    "#work with json\n",
    "import json\n",
    "\n",
    "#linear algebra\n",
    "import numpy as np \n",
    "\n",
    "#data processing\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', 25)\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "#for normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#For Statistics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#to create nueral network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "#setting the seed\n",
    "import random\n",
    "np.random.seed(1234)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DADOS BINANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Open            high             low           Close  \\\n",
      "Date                                                                         \n",
      "2017-08-17   4261.48000000   4485.39000000   4200.74000000   4285.08000000   \n",
      "2017-08-18   4285.08000000   4371.52000000   3938.77000000   4108.37000000   \n",
      "2017-08-19   4108.37000000   4184.69000000   3850.00000000   4139.98000000   \n",
      "2017-08-20   4120.98000000   4211.08000000   4032.62000000   4086.29000000   \n",
      "2017-08-21   4069.13000000   4119.62000000   3911.79000000   4016.00000000   \n",
      "...                    ...             ...             ...             ...   \n",
      "2024-03-28  69469.99000000  71552.06000000  68903.62000000  70780.60000000   \n",
      "2024-03-29  70780.60000000  70916.16000000  69009.00000000  69850.54000000   \n",
      "2024-03-30  69850.53000000  70321.10000000  69540.00000000  69582.18000000   \n",
      "2024-03-31  69582.17000000  71366.00000000  69562.99000000  71280.01000000   \n",
      "2024-04-01  71280.00000000  71288.23000000  68062.86000000  69649.80000000   \n",
      "\n",
      "                    volume     close_time   quote_asset_volume  \\\n",
      "Date                                                             \n",
      "2017-08-17    795.15037700  1503014399999     3454770.05073206   \n",
      "2017-08-18   1199.88826400  1503100799999     5086958.30617151   \n",
      "2017-08-19    381.30976300  1503187199999     1549483.73542151   \n",
      "2017-08-20    467.08302200  1503273599999     1930364.39032646   \n",
      "2017-08-21    691.74306000  1503359999999     2797231.71402728   \n",
      "...                    ...            ...                  ...   \n",
      "2024-03-28  35439.03239000  1711670399999  2500571208.00262760   \n",
      "2024-03-29  25445.08353000  1711756799999  1779608477.32245370   \n",
      "2024-03-30  13644.61142000  1711843199999   954555919.68557450   \n",
      "2024-03-31  19396.34433000  1711929599999  1367523707.26124510   \n",
      "2024-04-01  41445.32039000  1712015999999  2876486469.46552290   \n",
      "\n",
      "            number_of_trades taker_buy_base_asset_volume  \\\n",
      "Date                                                       \n",
      "2017-08-17              3427                616.24854100   \n",
      "2017-08-18              5233                972.86871000   \n",
      "2017-08-19              2153                274.33604200   \n",
      "2017-08-20              2321                376.79594700   \n",
      "2017-08-21              3972                557.35610700   \n",
      "...                      ...                         ...   \n",
      "2024-03-28           1799897              18007.26883000   \n",
      "2024-03-29           1522607              11839.09290000   \n",
      "2024-03-30           1110488               6423.89367000   \n",
      "2024-03-31           1181926              10142.47988000   \n",
      "2024-04-01           1893438              20211.15024000   \n",
      "\n",
      "           taker_buy_quote_asset_volume ignore  \n",
      "Date                                            \n",
      "2017-08-17             2678216.40060401      0  \n",
      "2017-08-18             4129123.31651808      0  \n",
      "2017-08-19             1118001.87008735      0  \n",
      "2017-08-20             1557401.33373730      0  \n",
      "2017-08-21             2255662.55315837      0  \n",
      "...                                 ...    ...  \n",
      "2024-03-28          1270852446.70774510      0  \n",
      "2024-03-29           828039936.68545330      0  \n",
      "2024-03-30           449436189.73361900      0  \n",
      "2024-03-31           715151368.94630210      0  \n",
      "2024-04-01          1402849944.78228910      0  \n",
      "\n",
      "[2420 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from binance.client import Client\n",
    "\n",
    "with open(\"keysBinance.json\", \"r\") as keys_json_file:\n",
    "        keys = json.load(keys_json_file)\n",
    "\n",
    "symbol = 'BTCUSDT'\n",
    "interval = '1d'\n",
    "\n",
    "# Start and end dates\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-04-01'\n",
    "\n",
    "# Dates for timestamps in milliseconds\n",
    "start_timestamp = int(pd.Timestamp(start_date).timestamp() * 1000)\n",
    "end_timestamp = int(pd.Timestamp(end_date).timestamp() * 1000)\n",
    "\n",
    "client = Client(keys[\"public_key\"], keys[\"secrect_key\"])\n",
    "\n",
    "# Initializing a list to store data\n",
    "all_klines = []\n",
    "\n",
    "# Maximum number of candles per request (Binance API limitation)\n",
    "limit = 1000\n",
    "\n",
    "# Making repeated calls to get all the data\n",
    "while True:\n",
    "    # Getting price history (candles) for the specified trading pair and range\n",
    "    klines = client.get_klines(symbol=symbol, interval=interval, startTime=start_timestamp, endTime=end_timestamp, limit=limit)\n",
    "\n",
    "    # Adding the data to the list\n",
    "    all_klines.extend(klines)\n",
    "\n",
    "    # If the number of candles returned is less than the threshold, we are done\n",
    "    if len(klines) < limit:\n",
    "        break\n",
    "\n",
    "    # Updating the start date for the next call\n",
    "    start_timestamp = int(klines[-1][0]) + 1\n",
    "\n",
    "# Data structure for DataFrame\n",
    "columns = ['Date', 'Open', 'high', 'low', 'Close', 'volume', 'close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "df1 = pd.DataFrame(all_klines, columns=columns)\n",
    "#df1.drop(columns=['close_time', 'quote_asset_volume','number_of_trades','taker_buy_base_asset_volume','taker_buy_quote_asset_volume','ignore'], inplace=True)\n",
    "# Converting timestamps to readable format\n",
    "df1['Date'] = pd.to_datetime(df1['Date'], unit='ms')\n",
    "df1.set_index('Date', inplace=True)\n",
    "df1.to_csv('dadosBinance.csv', index=True)\n",
    "\n",
    "# DataFrame with the dados\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train,valid,new_data,scaler,params,\n",
    "                scaled_data_train,scaled_data_valid,dados):    \n",
    "    \n",
    "    #creating the training set in the required format\n",
    "    #we will put together 60 days (offset) of data together and treat that as single input \n",
    "    #and the target value is the 'Close' price on the next day\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    dados_keys = list(dados.keys())\n",
    "    for i in range(params['offset'], len(train)):\n",
    "        arg = []\n",
    "        for j in range(0, len(dados_keys) - 1):\n",
    "            arg.append(scaled_data_train[i-params['offset']:i, j])\n",
    "        \n",
    "        x_train.append(arg)\n",
    "        y_train.append(scaled_data_train[i, dados_keys.index(params['target_column']) - 1])\n",
    "    \n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[2],x_train.shape[1]))\n",
    "    \n",
    "    #creating a new dataframe which will be used to create the test set\n",
    "    inputs = new_data[len(new_data) - len(valid) - params['offset']:].values\n",
    "    #inputs = inputs.reshape(-1,1)\n",
    "    inputs = scaler.transform(inputs)\n",
    "    \n",
    "    X_test, Y_test = [], []\n",
    "    for i in range(params['offset'],inputs.shape[0]):\n",
    "        arg = []\n",
    "        for j in range(0, len(dados_keys) - 1):\n",
    "            arg.append(inputs[i-params['offset']:i, j])\n",
    "        \n",
    "        X_test.append(arg)\n",
    "        Y_test.append(inputs[i, dados_keys.index(params['target_column']) - 1])\n",
    "    \n",
    "    X_test, Y_test = np.array(X_test), np.array(Y_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[2],X_test.shape[1]))    \n",
    "    \n",
    "    #create and fit the LSTM network\n",
    "    #we are building a general model here. This section of code will be used in further steps\n",
    "    #where we will check if only 1 hidden layer can give better results\n",
    "    #so an if-else loop is created to combat that situaiton\n",
    "    \n",
    "    if len(params[\"units\"]) != len(params[\"drop_rate\"]):\n",
    "        print(\"Units possui tamanho diferente de drop_rate. Saindo.\")\n",
    "        exit(1)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Adds the first layer\n",
    "    model.add(LSTM(units=params['units'][0], return_sequences=True, \n",
    "                    input_shape=(x_train.shape[1], len(dados_keys) - 1)))\n",
    "    model.add(Dropout(rate=params['drop_rate'][0]))\n",
    "\n",
    "    # Adds all the other layers (if they exists)\n",
    "    for i in range(1, len(params[\"units\"]) - 2):\n",
    "        model.add(LSTM(units=params['units'][i], return_sequences=True))\n",
    "        model.add(Dropout(rate=params['drop_rate'][i]))\n",
    "\n",
    "    model.add(LSTM(units=params['units'][-1]))\n",
    "    model.add(Dropout(rate=params['drop_rate'][-1]))\n",
    "    \n",
    "    # Finishes with the last Dense layer\n",
    "    model.add(Dense(params['no_of_outputs']))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    history = model.fit(x_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], \n",
    "                        verbose=1, validation_data=[X_test, Y_test])\n",
    "    \n",
    "    return model, history, X_test\n",
    "\n",
    "def get_accuracy(train,valid,new_data,tl, \n",
    "                 scaler,model,X_test,params,dados):\n",
    "    \n",
    "    dados_keys = list(dados.keys())\n",
    "    \n",
    "    closing_price = model.predict(X_test)\n",
    "    \n",
    "    new_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    new_scaler.min_ , new_scaler.scale_ = scaler.min_[1], scaler.scale_[1]\n",
    "    \n",
    "    closing_price = new_scaler.inverse_transform(closing_price)\n",
    "    \n",
    "    train = new_data[:tl]\n",
    "    valid = new_data[tl:]\n",
    "    valid['Predictions'] = closing_price\n",
    "    \n",
    "    #for plotting\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(train[dados[params['target_column']]])\n",
    "    plt.plot(valid[dados[params['target_column']]], label=params['target_label'])\n",
    "    plt.plot(valid['Predictions'], label=params['predict_label'])\n",
    "    plt.legend()\n",
    "    plt.savefig('Predicted-Close.png')\n",
    "    \n",
    "    #RMS error\n",
    "    rms = np.sqrt(np.mean(np.power((valid-closing_price),2)))\n",
    "    \n",
    "    #R-squared\n",
    "    y_true = valid[dados[params['target_column']]]\n",
    "    y_pred = valid['Predictions']\n",
    "    r = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return rms, r  \n",
    "   \n",
    "def run(data_df, params, dados):    \n",
    "    \n",
    "    #Plot the data and check if there are any unexpected anamolies(sudden spikes or dips)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(data_df[dados[params['target_column']]], label=params[\"history_label\"])\n",
    "    plt.title(params[\"history_label\"])\n",
    "    \n",
    "    #In our model, we will try to predict the future close price of a stock using the past\n",
    "    #open and close prices of that particular stock. So let's a create a new dataframe with \n",
    "    #only the 'Date', 'Open' and 'Close' price columns\n",
    "\n",
    "    #new_data = pd.DataFrame(index=range(0,len(data_df)),columns=dados.keys())\n",
    "    new_columns = []\n",
    "    for key in dados.keys():\n",
    "        new_columns.append(dados[key])\n",
    "\n",
    "    new_data = pd.DataFrame(index=range(0,len(data_df)),columns=new_columns)\n",
    "\n",
    "    for i in range(0,len(data_df)):\n",
    "        for key in dados.keys():\n",
    "            if key == \"Date\":\n",
    "                new_data[dados[\"Date\"]][i] = data_df.index[i]\n",
    "                continue\n",
    "\n",
    "            new_data[dados[key]][i] = data_df[dados[key]][i]\n",
    "        \n",
    "    #setting 'Date' column as index and dropping the original column\n",
    "    new_data.index = new_data.Date\n",
    "    new_data.drop(dados[\"Date\"], axis=1, inplace=True)\n",
    "    \n",
    "    #80% of the data is used as training set and 20% as test set\n",
    "    #'test set' here is referred to as 'validatation set'\n",
    "    frac = 0.8\n",
    "    tl = int(len(new_data)*frac)\n",
    "    \n",
    "    dataset = new_data.values\n",
    "    train = dataset[0:tl,:]\n",
    "    valid = dataset[tl:,:]\n",
    "    \n",
    "    #Normalizing the data\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(train)\n",
    "    scaled_data_train = scaler.transform(train)\n",
    "    scaled_data_valid = scaler.transform(valid)\n",
    "    \n",
    "    #building the LSTM model\n",
    "    model, history, X_test = build_model(train,valid,new_data,scaler,params,\n",
    "                                         scaled_data_train,scaled_data_valid,dados)\n",
    "    \n",
    "    #getting the 'RMSE error' and 'R-squared value'\n",
    "    rms, r = get_accuracy(train,valid,new_data,tl,\n",
    "                          scaler,model,X_test, params, dados)\n",
    "    \n",
    "    print(f'RMS: {rms}, R-square: {r}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #read dados json file\n",
    "    with open(\"dados.json\", \"r\") as json_file:\n",
    "        dados = json.load(json_file)\n",
    "    \n",
    "    #Readind the data and changing it into pandas dataframe\n",
    "    data_df = pd.read_csv('dadosBinance.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "    #Defining the initial parameters of the model    \n",
    "    params = {'offset':60,\n",
    "             'units': [32, 32, 32],\n",
    "             'drop_rate': [0, 0, 0],\n",
    "             'batch_size':5,\n",
    "             'epochs':20,\n",
    "             'no_of_outputs':1,\n",
    "             'target_column': 'parameter2',\n",
    "             'target_label': 'Actual Close Price',\n",
    "             'predict_label': 'Predicted Close Price',\n",
    "             'history_label': 'Close Price history'\n",
    "             }    \n",
    "\n",
    "    run(data_df, params, dados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
